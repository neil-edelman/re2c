\documentclass[]{article}
\usepackage[margin=2cm]{geometry}

\usepackage[utf8]{inputenc}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage[justification=centering,labelfont=bf]{caption}
\usepackage{url}
\usepackage{multicol}
\usepackage{setspace}
\usepackage{placeins}

\usepackage[noline, noend, nofillcomment, linesnumbered]{algorithm2e}
\SetArgSty{textnormal}
\SetCommentSty{texttt}
\SetNoFillComment
\SetNlSty{textnormal}{}{}
\renewcommand{\algorithmcfname}{Algorithm}
\def\Centerline#1{\begin{centering}{#1}\end{centering}}\SetAlgoCaptionLayout{Centerline}
\SetNlSkip{0.8em}
\SetNlSty{texttt}{}{}
\let\oldnl\nl% Store \nl in \oldnl
\newcommand{\nonl}{\renewcommand{\nl}{\let\nl\oldnl}}% Remove line number for one line

\setlist{nosep}

\newtheorem{definition}{Definition}

\newenvironment{Xfig}
    {\par\medskip\noindent\minipage{\linewidth}\begin{center}}
    {\end{center}\endminipage\par\medskip}
\newenvironment{Xtab}
    {\par\medskip\noindent\minipage{\linewidth}\begin{center}}
    {\end{center}\endminipage\par\medskip}

\setlength{\parindent}{0pt}
\setlength{\belowcaptionskip}{-1em}

\newcommand*{\Xbar}[1]{\overline{#1}}

\newcommand{\Xl}{\langle}
\newcommand{\Xr}{\rangle}
\newcommand{\Xm}{\langle\!\rangle}
\newcommand{\Xset}{\!\leftarrow\!}
\newcommand{\Xund}{\rule{.5em}{.5pt}}
\newcommand{\Xlb}{[\![}
\newcommand{\Xrb}{]\!]}
\newcommand{\Xmap}{\!\mapsto\!}
\newcommand{\XB}{\mathcal{B}}
\newcommand{\XD}{\mathcal{D}}
\newcommand{\XE}{\mathcal{E}}
\newcommand{\XF}{\mathcal{F}}
\newcommand{\XI}{\mathcal{I}}
\newcommand{\XPT}{\XP\!\XT}
\newcommand{\XIT}{\XI\!\XT}
\newcommand{\XIR}{\XI\!\XR}
\newcommand{\XL}{\mathcal{L}}
\newcommand{\XN}{\mathcal{N}}
\newcommand{\XM}{\mathcal{M}}
\newcommand{\XO}{\mathcal{O}}
\newcommand{\XP}{\mathcal{P}}
\newcommand{\XR}{\mathcal{R}}
\newcommand{\XS}{\mathcal{S}}
\newcommand{\XT}{\mathcal{T}}
\newcommand{\XX}{\mathcal{X}}
\newcommand{\YB}{\mathbb{B}}
\newcommand{\YC}{\mathbb{C}}
\newcommand{\YK}{\mathbb{K}}
\newcommand{\YF}{\mathbb{F}}
\newcommand{\YN}{\mathbb{N}}
\newcommand{\YO}{\mathbb{O}}
\newcommand{\YT}{\mathbb{T}}
\newcommand{\YQ}{\mathbb{Q}}
\newcommand{\YP}{\mathbb{P}}
\newcommand{\YZ}{\mathbb{Z}}
\newcommand{\PT}{PT}
\newcommand{\PE}{P\!E}
\newcommand{\PR}{P\!R}
\newcommand{\IPT}{I\!PT}
\newcommand{\IRE}{I\!RE}
\newcommand{\Eps}{E\!ps}
\newcommand{\Sym}{S\!ym}
\newcommand{\Alt}{Alt}
\newcommand{\Cat}{C\!at}
\newcommand{\Rep}{Rep}
\newcommand{\Xop}{X\!op}
\newcommand{\fix}{f\!ix}
\newcommand{\isub}{\mathit{isub}}
\newcommand{\esub}{\mathit{esub}}


\begin{document}

\title{A closer look at TDFA}
%\title{TDFA -- Fast Submatch Extraction in Regular Expressions}
%\title{Fast submatch extraction with lookahead-TDFA}
\author{
    Angelo Borsotti \\
    \texttt{\small{angelo.borsotti@mail.polimi.it}}
\and
    Ulya Trofimovich \\
    \texttt{\small{skvadrik@gmail.com}}
}
\date{2022}

\maketitle

\begin{abstract}
In this paper we revisit the algorithm for fast submatch extraction in regular expressions
based on tagged deterministic finite automata.
We give a comprehensive description of the algorithm with pseudocode,
covering a few practical optimizations.
All transformations from a regular expression to an optimized automaton are explained on a step-by-step example.
We consider both ahead-of-time and just-in-time determinization
and describe variants of the algorithm that are better suited to each setting.
Finally, we benchmark the algorithm and show that it is very fast in practice.
Our research is based on two independent implementations:
an open-source lexer generator RE2C
and an experimental Java library.
\end{abstract}

\section*{Introduction}
This paper describes tagged deterministic finite automata (TDFA).
It is primarily targeted at practical readers who want to implement fast submatch extraction in a lexer generator or a regular expression library.
Most of the theory in the paper is not new,
but the previous descriptions are incomplete and lack important details.
%
Here is a brief history of TDFA development.
In 2000 Laurikari published the original paper \cite{Lau00}.
In 2007 Kuklewicz implemented the algorithm in a Haskell library with POSIX longest-match disambiguation [Regex-TDFA].
In 2016 Trofimovich described TDFA with lookahead \cite{Tro17} and implemented them in the open-source lexer generator RE2C \cite{RE2C}.
In 2017 Borsotti implemented TDFA in a Java library (unpublished).
In 2019 Borsotti and Trofimovich adapted POSIX disambiguation algorithm by Okui and Suzuki to TDFA
and showed that it is faster than Kuklewicz disambiguation \cite{BorTro19}.
%
Now, after TDFA have been used in real-world programs for a while,
we want to provide a comprehensive description of the algorithm and make it easier to implement.
In this paper we also present TDFA modification that is better suited to regular expression libraries.
Before diving into details, we recall a few key concepts.
\\

%\paragraph{Regular expressions and submatch extraction.}
\emph{Regular expressions} (RE) are a notation for describing sets of strings known as regular languages, or Type-3 languages in the Chomsky hierarchy.
They were first defined by Kleene \cite{Kle51} as sets of strings constructed from the alphabet symbols and the empty word via the application of three basic operations: concatenation, alternative and iteration.
Later RE were formalized via the notion of Kleene algebra \cite{Koz94}.
In practice RE have many extensions that vary in complexity and expressive power.
We focus on \emph{submatch extraction} extension that allows one to identify substrings matching specific parts of RE.
Recall the difference between \emph{recognition} and \emph{parsing}:
to recognize a string one needs to determine if it belongs to the language,
but to parse a string one also needs to find its derivation in the language grammar (to build a parse tree).
Submatch extraction is similar to parsing, except it does not require a full derivation,
therefore generic parsing algorithms are too heavyweight for it.
\\

%\paragraph{Non-determinism.}
The recognition problem for RE can be solved with finite automata.
Both nondeterministic finite automata (NFA) and deterministic finite automata (DFA)
recognize a string in linear time depending on its length.
In practice DFA are faster because they follow a single path,
while NFA have to track multiple paths simultaneously.
NFA can be converted to DFA using the \emph{determinization} procedure,
but the resulting DFA may be exponentially larger than NFA.
%
Submatch extraction is easier with NFA because they track submatch values separately for different paths,
while DFA collapse all paths into one,
causing conflicts between submatch values.
To resolve the conflicts DFA must keep track of all conflicting values in each state;
they are augmented with \emph{registers} and operations that update register values on transitions from one state to another.
Constructing augmented DFA with minimum runtime overhead is the main subject of this paper.
\\

%\paragraph{Ambiguity.}
It is important to differentiate \emph{non-determinism} and \emph{ambiguity}.
Non-determinism is the existense of multiple possibilities during parsing,
all of which need to be considered, but some may lead to a deadend.
Ambiguity is the existense of multiple different ways to parse the input in principle.
Non-determinism is a temporary obstacle that a parsing algorithm has to deal with,
and ambiguity is a genuine property of grammar (or sometimes a language).
%
Ambiguity is as much of a problem for NFA as for DFA, as well as any parsing algorithm.
To deal with it, one needs a way to choose between ambiguous parses --- a \emph{disambiguation policy}.
The two widely used policies are Perl leftmost-greedy policy and POSIX longest-match policy.
The latter is harder to implement, although it is sometimes considered more intuitive.
Some RE engines provide other ways to resolve ambiguity, such as user-defined precedence rules,
but these are ad-hoc, error-prone and often difficult to reason about.
POSIX disambiguation is covered in a separate paper \cite{BorTro19};
here it suffices to say that TDFA can be parameterized over a disambiguation policy
and there is no runtime overhead on disambiguation (it is built into TDFA structure).
\\

%\paragraph{Lexer generators vs libraries.}
The choice of a matching algorithm depends on a particular setting.
RE engines can be roughly divided in two categories: libraries and lexer generators.
Libraries perform interpretation or just-in-time compilation of RE ---
they face a tradeoff between the time spent on preprocessing and the time spent on matching.
Lexer generators, on the other hand, perform ahead-of-time compilation and do not have such a tradeoff.
Consequently, libraries use a variety of algorithms ranging from recursive backtracking to NFA, DFA, string searching or some combination of the above;
while lexer generators almost always use DFA and may spend considerable time on optimizations in order to emit better code.
TDFA can be used in both settings, but their determinisitc nature and low runtime overhead make them particularly well-suited for lexer generators.
We have used two independent TDFA implementations: an open-source lexer generator RE2C that generates C code
and an experimental runtime Java library.
\\

%\paragraph{Representation of submatch results.}
The speed of a submatch extraction algorithm depends on the representation of submatch results.
A \emph{parse tree} is the most generic representation; it precisely reflects a derivation.
In practice a more lightweight representation is widely used:
either a \emph{list of offsets} or the \emph{last offset} for each submatch position in RE
(the latter is used in POSIX \texttt{regexec} function).
%
Our algorithm can extract both offset lists or single offsets,
and the choice is individual for each submatch position.
We primarily focus on the single-offset representation
because it permits various optimizations described in section \ref{section_optimizations}.
%
As for parse trees, we use \emph{tagged strings} which store trees in serialized form and have minimal runtime overhead
(our goal is to compare the time spent on submatch extraction, not tree construction).
%
%In section \ref{subsection_extensions_regless} we describe \emph{registerless} TDFA
%that are better suited to full parsing than TDFA with registers.
The benchmarks in section \ref{section_evaluation} cover different representations and RE with different submatch density.
\\

%\paragraph{Automata related to TDFA.}
In this paper we focus on TDFA with \emph{lookahead} described by Trofimovich \cite{Tro17}, not the original TDFA described by Laurikari \cite{Lau00}.
The two types of automata are called TDFA(1) and TDFA(0) by analogy with LR(1) and LR(0) automata:
TDFA(1) utilize the lookahead symbol during determinization, which allows them to reduce non-determinism and use fewer registers and register operations than TDFA(0).
We benchmark TDFA(1) against TDFA(0), as well as two other types of automata: sta-DFA \cite{Cho18} and DSST \cite{Gra15}.
Sta-DFA are very similar to TDFA, but they have register operations in states rather than on transitions (and do not use lookahead).
DSST stands for Deterministic Streaming String Transducers; these are more distant relatives to TDFA,
better suited to string rewriting and full parsing.
We also benchmark TDFA against Ragel \cite{Ragel}, which uses simple DFA with ad-hoc user-defined actions.
\\

The rest of the paper is structured as follows.
Section \ref{section_tnfa} defines RE and their conversion to nondeterministic automata.
Section \ref{section_tdfa} describes determinization and formally defines TDFA.
Section \ref{section_implementation} describes optimizations.
Section \ref{section_example} gives a step-by-step example of all transtormations from a RE to an optimized TDFA.
Section \ref{section_regless} describes \emph{registerless} TDFA and their application to just-in-time determinization.
Section \ref{section_evaluation} provides benchmarks and comparison with other algorithms.
Finally, section \ref{section_conclusions} contains conclusions and ideas for future work.

\section*{A note on notation}

For brevity, pseudocode in this paper assumes that
function arguments are passed by reference and modifications to them are visible in the calling function.

\pagebreak

\section{TNFA}\label{section_tnfa}

%In this section we formalize TDFA and prove that the algorithm is correct.
%We do that in the following steps.
%First, we formalize RE.
%Then we show how to convert RE to NFA that closely mirrors the structure of RE, and therefore preserves submatch information and ambiguity in it.
%Then we show how to simulate NFA and obtain submatch results.
%Since NFA has the same structure and submatch information as the original RE,
%and since the simulation procedure is a trivial modification of the canonical simulation,
%it is easy to see that it yields correct submatch values.
%The only tricky part is disambiguation, but in this paper we take it for granted ---
%POSIX disambiguation is described in great detail in another paper [BoRTro19],
%and leftmost greedy disambiguation is trivial since by definition it can be implemented with a simple leftmost depth-first search over the NFA.
%Then we show how to convert NFA to DFA and how to execute DFA.
%Finally, we show that DFA execution yields the same results as NFA simulation, which proves that the algorithm is correct.

In this section we define RE and TNFA, show how to construct TNFA from RE and how to match a string.

\begin{definition}
Regular expressions (RE) over finite alphabet $\Sigma$ are:
\begin{enumerate}
    \item
      Empty RE $\epsilon$,
      unit RE $a \in \Sigma$
      and tag $t \in \YN$.
    \item Alternative $e_1 | e_2$,
      concatenation $e_1 e_2$ and
      repetition $e_1^{n, m}$ $(0 \!\leq\! n \!\leq\! m \!\leq\! \infty)$
      where $e_1$ and $e_2$ are RE over $\Sigma$.
\end{enumerate}
\end{definition}

Tags are submatch markers that can be placed anywhere in RE.
They may be standalone or correspond to capturing parentheses
(the correspondence may be nontrivial, e.g. POSIX capturing groups require insertion of hierarchical tags \cite{BorTro19}).
Generalized repetition $e^{n, m}$ can be bounded ($m < \infty$) or unbounded ($m = \infty$).
Unbounded repetition $e^{0,\infty}$ is the canonical Kleene iteration, shortened as $e^*$.
Bounded repetition is often desugared via concatenation, but in the presence of tags that could change submatch information in RE.

\begin{definition} \label{def_tnfa}
Tagged Nondeterministic Finite Automaton (TNFA)
is a structure $(\Sigma, T, Q, q_0, q_f, \Delta)$, where:
\begin{itemize}
    \item[] $\Sigma$ is a finite set of symbols (alphabet)
    \item[] $T \subset \YN$ is a finite set of tags
    \item[] $Q$ is a finite set of states with initial state $q_0$ and final state $q_f$
    \item[] $\Delta$ is a transition relation that contains transitions of two kinds:
    \begin{itemize}
        \item[] transitions on alphabet symbols $(q, a, p)$ where $q, p \in Q$ and $a \in \Sigma$
        \item[] optionally tagged $\epsilon$-transitions with priority $(q, i, t, p)$ where $q, p \in Q$, $i \in \YN$ and $t \in T \cup \Xbar{T} \cup \{\epsilon\}$
    \end{itemize}
\end{itemize}
\end{definition}

TNFA is in essence a non-deterministic finite state transducer with input alphabet $\Sigma$ and output alphabet $\Sigma \cup T \cup \Xbar{T}$, it rewrites symbolic strings into tagged strings.
$\Xbar{T} = \{-t \mid t \in T\}$ is the set of all negative tags, which represent the absence of match: they appear whenever there is a way to bypass a tagged subexpression in RE,
such as alternative or repetition with zero lower bound.
Explicit representation of negative match serves a few purposes:
it prevents stale submatch values from propagating to subsequent iterations,
it spares the need to initialize tags,
and it is required by POSIX disambiguation [BorTro19].
Priorities are used for transition ordering during $\epsilon$-closure construction.
%
Algorithm \ref{alg_tnfa} on page \pageref{alg_tnfa} shows TNFA construction:
it performs top-down structural recursion on RE, passing the final state on recursive descent into subexpressions
and using it to connect subautomata.
This is similar to Thompson's construction, except that non-essential $\epsilon$-transitions are removed and tagged transitions are added.
The resulting automaton mirrors the structure of RE and preserves submatch information and ambiguity in it.

\begin{algorithm}[] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\begin{multicols}{2}
\setstretch{0.9}
\small
\Indm

\nonl\Fn {$\underline{simulation \big( (\Sigma, T, Q, q_0, q_f, \Delta), \; a_1 \hdots a_n \big)} \smallskip$} {
    $m_0:$ vector of offsets of size $|T|$ \;
    $C = \{ (q_0, m_0) \}$ \;
    \BlankLine
    \For {$k = \overline{1, n}$} {
        $C = epsilon \Xund closure(C, \Delta, q_f, k)$ \;
        $C = step \Xund on \Xund symbol(C, \Delta, a_k)$ \;
        \lIf {$C = \emptyset$} {
            \Return $\varnothing$
        }
    }
    \BlankLine
    $C = epsilon \Xund closure(C, \Delta, q_f, n)$ \;
    \BlankLine
    \lIf {$\exists (q, m)$ in $C \mid q = q_f$} {
        \Return $m$
    } \lElse {
        \Return $\varnothing$
    }
}
\vspace{2em}

\nonl\Fn {$\underline{step \Xund on \Xund symbol \big( C, \Delta, a \big)} \smallskip$} {
    \Return $\{ (p, m) \mid (q, m)$ in $C$ and $(q, a, p) \in \Delta \}$ \;
}

\vfill\null
\columnbreak

\nonl\Fn {$\underline{epsilon \Xund closure \big( C, \Delta, q_f, k \big)} \smallskip$} {
    $C':$ empty sequence of configurations \;
    \BlankLine
    \For {$(q, m)$ in $C$ in reverse order} {
        push $(q, m)$ on stack \;
    }
    \BlankLine
    \While {stack is not empty} {
        pop $(q, m)$ from stack \;
        append $(q, m)$ to $C'$ \;
        \BlankLine
        \For {each $(q, i, t, p) \in \Delta$ ordered by priority $i$} {
            \lIf {$t > 0$} {
                $m[t] = k$
            } \lElse {
                $m[-t] = \mathbf{n}$
            }
            %\BlankLine
            \If {configuration with state $p$ is not in $C'$} {
                push $(p, m)$ on stack \;
            }
        }
    }
    \BlankLine
    \Return $\{ (q, m)$ in $C' \mid q = q_f$ or \\
        \hphantom{\Return $\{ (q, m)$ in $C' \mid$} $\exists (q, a, \Xund) \in \Delta$ where $a \in \Sigma \}$  \;
}

\vfill\null

\end{multicols}
\vspace{1em}
\caption{TNFA simulation.}\label{alg_simulation}
\end{algorithm}

Algorithm \ref{alg_simulation} defines TNFA simulation on a string.
It starts with a single configuration $(q_0, m_0)$ consisting of the initial state $q_0$ and an empty vector of tag values,
and loops over the input symbols until all of them are matched or the configuration set becomes empty, indicating match failure.
At each step the algorithm constructs $\epsilon$-closure of the current configuration set, updating tag values along the way, and steps on transitions labeled with the current input symbol.
Finally, if all symbols have been matched and there is a configuration with the final state $q_f$, the algorithm terminates successfully and returns the final vector of tag values.
Otherwise it returns a failure.
The algorithm uses leftmost greedy disambiguation; POSIX disambiguation is more complex and requires a different $\epsilon$-closure algorithm [BorTro19].
Figure \ref{fig:tdfa_construction} in section \ref{section_example} shows an example of TNFA simulation.

%\FloatBarrier

\begin{algorithm}[] \DontPrintSemicolon \SetKwProg{Fn}{}{}{}
\begin{multicols}{2}

    \newcommand \retonfa {tn\!f\!a}%{\XN}
    \newcommand \ntag {ntags}
    \Indm
    \small
    \setstretch{1.1}

    \nonl\Fn {$\underline{\retonfa(e, q_f)} \smallskip$} {
    %\Indm

    \If {$e = \epsilon$} {
        \Return $(\Sigma, \emptyset, \{q_f\}, q_f, q_f, \emptyset)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = a \in \Sigma$} {
        \Return $(\Sigma, \emptyset, \{q_0,q_f\}, q_0, q_f, \{(q_0, a, q_f)\})$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = t \in \YN$} {
        \Return $(\Sigma, \{t\}, \{q_0, q_f\}, q_0, q_f, \{(q_0, 1, t, q_f)\})$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1 \cdot e_2$} {
        $(\Sigma, T_2, Q_2, q_2, q_f, \Delta_2) = \retonfa (e_2, q_f)$ \;
        $(\Sigma, T_1, Q_1, q_1, q_2, \Delta_1) = \retonfa (e_1, q_2)$ \;
        \Return $(\Sigma, T_1 \cup T_2, Q_1 \cup Q_2, q_1, q_f, \Delta_1 \cup \Delta_2)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1 \mid e_2$} {
        $(\Sigma, T_2, Q_2, q_2, q_f, \Delta_2) = \retonfa (e_2, q_f)$ \;
        $(\Sigma, T_2, Q'_2, q'_2, q_f, \Delta'_2) = \ntag (T_2, q_f)$ \;
        $(\Sigma, T_1, Q_1, q_1, q'_2, \Delta_1) = \retonfa (e_2, q'_2)$ \;
        $(\Sigma, T_1, Q'_1, q'_1, q_2, \Delta'_1) = \ntag (T_1, q_2)$ \;
        $Q = Q_1 \cup Q'_1 \cup Q_2 \cup Q'_2 \cup \{q_0\}$ \;
        $\Delta = \Delta_1 \cup \Delta'_1 \cup \Delta_2 \cup \Delta'_2 \cup \{ (q_0,1,\epsilon,q_1), (q_0,2,\epsilon,q'_1) \}$ \;
        \Return $(\Sigma, T_1 \cup T_2, Q, q_0, q_f, \Delta)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1^{n, m} \mid_{1 < n \leq m \leq \infty}$} {
        $(\Sigma, T_1, Q_1, q_2, q_f, \Delta_1) = \retonfa(e_1^{n\!-\!1, m\!-\!1}, q_f)$ \;
        $(\Sigma, T_2, Q_2, q_1, q_2, \Delta_2) = \retonfa(e_1, q_2)$ \;
        \Return $(\Sigma, T_1 \cup T_2, Q_1 \cup Q_2, q_1, q_f, \Delta_1 \cup \Delta_2)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1^{1, m} \mid_{1 < m < \infty}$} {
        \lIf {$m = 1$} {
            \Return $\retonfa (e_1, q_f)$
        }
        $(\Sigma, T_1, Q_1, q_1, q_f, \Delta_1) = \retonfa (e_1^{1, m\!-\!1}, q_f)$ \;
        $(\Sigma, T_2, Q_2, q_0, q_2, \Delta_2) = \retonfa (e_1, q_1)$ \;
        $\Delta = \Delta_1 \cup \Delta_2 \cup \{ (q_1, 1, \epsilon, q_f), (q_1, 2, \epsilon, q_2) \}$ \;
        \Return $(\Sigma, T_1 \cup T_2, Q_1 \cup Q_2, q_0, q_f, \Delta)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1^{0, m}$} {
        $(\Sigma, T_1, Q_1, q_1, q_f, \Delta_1) = \retonfa (e_1^{1, m}, q_f)$ \;
        $(\Sigma, T_1, Q'_1, q'_1, q_f, \Delta'_1) = \ntag (T_1, q_f)$ \;
        $Q = Q_1 \cup Q'_1 \cup \{q_0\}$ \;
        $\Delta = \Delta_1 \cup \Delta'_1 \cup \{ (q_0, 1, \epsilon, q_1), (q_0, 2, \epsilon, q'_1) \}$ \;
        \Return $(\Sigma, T_1, Q, q_0, q_f, \Delta)$
    }
    \BlankLine
    \BlankLine

    \ElseIf {$e = e_1^{1, \infty}$} {
        $(\Sigma, T_1, Q_1, q_0, q_1, \Delta_1) = \retonfa (e_1, q_1)$ \;
        $Q = Q_1 \cup \{q_f\}$ \;
        $\Delta = \Delta_1 \cup \{ (q_1, 1, \epsilon, q_0), (q_1, 2, \epsilon, q_f) \}$ \;
        \Return $(\Sigma, T_1, Q, q_0, q_f, \Delta)$
    }
    }
    \vspace{2em}

    \nonl\Fn {$\underline{\ntag(T, q_f)} \smallskip$} {
        \Indp
        $\{ t_i \}_{i=1}^n = T$ \;
        $Q = \{q_i\}_{i=0}^n$ where $q_n = q_f$ \;
        $\Delta = \{ (q_{i-1}, 1, -t_i, q_i) \}_{i=1}^n$ \;
        \Return $(\Sigma, T, Q, q_0, q_f, \Delta)$ \;
    }

    \vfill\null

\columnbreak

    \nonl \includegraphics[width=\linewidth]{img/tnfa_construction.pdf}

\end{multicols}
\vspace{1em}
\caption{TNFA construction.}\label{alg_tnfa}
\end{algorithm}


\section{TDFA}\label{section_tdfa}

In this section we define TDFA and show how to convert TNFA to TDFA.

\begin{definition} \label{def_tnfa}
Tagged Deterministic Finite Automaton (TDFA)
is a structure $(\Sigma, T, S, S_f, s_0, R, r_{\!f}, \delta, \varphi)$, where:
\begin{itemize}
    \item[] $\Sigma$ is a finite set of symbols (alphabet)
    \item[] $T \subset \YN$ is a finite set of tags
    \item[] $S$ is a finite set of states with initial state $s_0$ and a subset of final states $S_f \subseteq S$
    \item[] $R \subset \YN$ is a finite set of registers with a vector of final registers $r_{\!f}$ (one per tag)
    \item[] $\delta : S \times \Sigma \rightarrow S \times \YO^*$ is a transition function
    \item[] $\varphi : S_f \rightarrow \YO^*$ is a final function
    \medskip
    \item[] where $\YO$ is a set of register operations of the following types:
    \begin{itemize}
        \item[] set register $i$ to nil or to the current position: $i \leftarrow v$, where $v \in \{\mathbf{n}, \mathbf{p}\}$
        \item[] copy register $j$ to register $i$: $i \leftarrow j$
        \item[] copy register $j$ to register $i$ and append history: $i \leftarrow j \cdot h$, where $h$ is a string over $\{\mathbf{n}, \mathbf{p}\}$
    \end{itemize}
\end{itemize}
\end{definition}

Compared to an ordinary DFA, TDFA is extended with a set of tags $T$,
a set of registers $R$ with one final register per tag,
and register operations that are attributed to transitions and final states (the $\delta$ and $\varphi$ functions).
$\YO^*$ denotes the set of all sequences of operations over $\YO$.
Operations can be of three types: set, copy, append.
Set operations are used for \emph{single-valued} tags (those represented with a single offset),
append operations are used for \emph{multi-valued} tags (those represented with an offset list), and
copy operations are used for all tags.
The decision which tags are single-valued and which ones are multi-valued is arbitrary and individual for each tag.
It may be based on whether the tag is under repetition, but not necessarily.
%
Register values are denoted by special symbols $\mathbf{n}$ and $\mathbf{p}$, which mean \emph{nil} and the \emph{current position} (offset from the beginning of the input string).
\\

Recall the canonical determinization algorithm that is based on powerset construction:
NFA is simulated on all possible strings,
and the subset of NFA states at each step of the simulation forms a new DFA state,
which is either mapped to an existing identical state or added to the growing set of DFA states.
Since the number of different subsets of NFA states is finite, determinization eventually terminates.
%
The presence of tags complicates things: it is necessary to track tag values, which depend on the offset that increases at every step.
This makes the usual powerset construction impossible: DFA states augmented with tag values are different and cannot be mapped.
As a result the set of states grows indefinitely and determinization does not terminate.
%
To address this problem, Laurikari used indirection: instead of storing tag values in TDFA states, he stored value locations --- \emph{registers}.
As long as two TDFA states have the same registers, the actual values in registers do not matter:
they change dynamically at runtime (during TDFA execution), but they do not affect TDFA structure.
A similar approach was used by Grathwohl [Gra15], who described it as splitting the information contained in a value into static and dynamic parts.
The indirection is not free: it comes at the cost of runtime operations that update register values.
But it solves the termination problem, as the required number of registers is finite, unlike the number of possible register values.
\\

From the standpoint of determinization TDFA state is a pair.
The first component is a set of configurations $(q, r, l)$ where
$q$ is a TNFA state,
$r$ is a vector of registers (one per tag) and
$l$ is a sequence of tags.
Unlike TNFA simulation that updates tag values immediately when it encounters a tagged transition,
determinization delays the application of tags until the next step.
%
It records tag sequences along TNFA paths in the $\epsilon$-closure,
but instead of applying them to the current transition,
it stores them in configurations of the new TDFA state
and later applies to the outgoing transitions.
%
%but the corresponding register operations are attributed to transitions going out of TDFA state, not the incoming one.
This allows filtering tags by the lookahead symbol:
confgurations that have no TNFA transitions on the lookahead symbol
do not contribute any register operations to TDFA transition on that symbol.
The use of the lookahead symbol is what distingushes TDFA(1) from TDFA(0) [Tro17];
it considerably reduces the number of operations and registers.
During $\epsilon$-closure construction configurations are extended to four components $(q, r, h, l)$
where $h$ is the sequence of tags inherited from the origin TDFA state
and $l$ is the new sequence constructed by the $\epsilon$-closure.
\\

The second component of TDFA state is precedence information.
It is needed for ambiguity resolution:
if some TNFA state in the $\epsilon$-closure can be reached by different paths, one path must be preferred over the others.
This affects submatch extraction, as the paths may have different tags.
The form of precedence information depends on the disambiguation policy;
we keep the details encapsulated in the $precedence$ function,
so that algorithm \ref{alg_tdfa} can be adapted to different policies without the need to change its structure.
%
In the case of leftmost greedy policy precedence information is a vector of TNFA states that represents an order on configurations:
$step \Xund on \Xund symbol$ uses it to construct the initial closure,
and $epsilon \Xund closure$ performs depth-first search following transitions from left to right.
%
POSIX policy is more complex, and we do not include pseudocode for it in this paper
(see another paper [BorTro19] for a detailed explanation).
\\

Algorithm \ref{alg_tdfa} works as follows.
The main function $determinization$ starts by allocating initial registers $r_0$ from $1$ to $|T|$ and final registers $r_{\!f}$ from $|T| + 1$ to $2|T|$.
It constructs the initial TDFA state $s_0$ as the $\epsilon$-closure of the initial configuration $(q_0, r_0, \epsilon, \epsilon)$.
The initial state $s_0$ is added to the set of states $S$ and the algorithm loops over states in $S$, possibly adding new states on each iteration.
For each state $s$ the algorithm explores outgoing transitions on all alphabet symbols.
Function $step \Xund on \Xund symbol$ follows transitions marked with a given symbol,
and  function $epsilon \Xund closure$ constructs $\epsilon$-closure $C$, recording tag sequences along each fragment of TNFA path.
%
The set of configurations in the $\epsilon$-closure forms a new TDFA state $s'$.
Function $transition \Xund regops$ uses the $h$-components of configurations in $C$ to construct register operations on transition from $s$ to $s'$.
%
The same register is allocated for all outgoing transitions with identical operation right-hand-sides,
but different tags do not share registers,
and vacant registers from other TDFA states are not reused
(these rules ensure that there are no artificial dependencies between registers, which makes optimizations easier without the need to construct SSA [SSA]).
%
The new state $s'$ is inserted into the set of states $S$:
function $add \Xund state$ first tries to find an identical state in $S$;
if that fails, it looks for a state that can be mapped to $s'$;
if that also fails, $s'$ is added to $S$.
%
If the new state contains the final TNFA state, it is added to $S_f$,
and the $final \Xund regops$ function constructs register operations for the final quasi-transition
(called so because it does not consume input characters and gets executed only at the end of match).
\\

TDFA states are considered identical if both components (configuration set and precedence) coincide.
States that are not identical, but differ only in registers, can be mapped, provided that there is a bijection between registers.
Function $map$ attempts to construct such a bijection $M$:
for every tag, and for each pair of configurations
it adds the corresponding pair of registers to $M$.
If either of the two registers is already mapped to some other register, bijection cannot be constructed.
For single-valued tags mapping ignores configurations that have the tag in the lookahead sequence ---
every transition out of TDFA state overwrites tag value with a set operation, making the current register values obsolete.
For multi-valued tags this optimization is not possible, because append operations do not overwrite previous values.
If the mapping has been constructed successfully, $map$ updates register operations:
for each pair of registers in $M$ it adds a copy operation,
unless the left-hand-side is already updated by a set or append operation,
in which case it replaces left-hand-side with the register it is mapped to.
The operations are topologically sorted ($topological \Xund sort$ is defined on page \pageref{alg_opt});
in the presence of copy and append operations this is necessary to ensure that old register values are used before they are updated.
Topological sort ignores trivial cycles such as append operation $i \leftarrow i \cdot h$,
but if there are nontrivial cycles the mapping is rejected
(handling such cycles requires a temporary register, which makes control flow more complex for optimizations).
\\

After determinization is done, the information in TDFA states is erased --- it is no longer needed for TDFA execution.
States are just atomic values that can be represented with integer numbers.
Disambiguation decisions are embedded in TDFA structure; there is no explicit disambiguation at runtime.
The only runtime overhead on submatch extraction is the execution of register operations on transitions.
TDFA may have more states than an ordinary DFA for the same RE without tags, because states that can be mapped in a DFA cannot always be mapped in a TDFA.
Minimization can reduce the number of states,
especially if it is applied after register optimizations that can get rid of many operations and make more states compatible.
We focus on optimizations in section \ref{section_optimizations}.
%The difference between TDFA and an ordinary DFA is the presence of registers and register operations on transitions.
\\

\begin{algorithm}[] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\begin{multicols}{2}
\setstretch{0.9}
\small
\Indm

\nonl\Fn {$\underline{determinization \big( \Sigma, T, Q, q_0, q_f, \Delta \big) } \smallskip$} {
    $S, S_f:$ empty sets of states \;
    $\delta:$ undefined transition function \;
    $\varphi:$ undefined final function \;
    %$\rho:$ precedence function \;
    %$\delta, \varphi, \rho:$ transition, final and precedence functions \;
    $r_0 = \{1, ...\,, |T|\},\; r_{\!f} = \{|T|\!+\!1, ...\,, 2|T|\},\; R = r_0 \cup r_{\!f}$ \;
    \BlankLine
    $C = epsilon \Xund closure (\{( q_0, r_0, \epsilon, \epsilon )\})$ \;
    $P = precedence (C)$ \;
    $s_0 = add \Xund state (S, S_f, r_{\!f}, \varphi, C, P, \epsilon)$ \;
    \BlankLine
    \For {each state $s \in S$} {
        $V:$ map from tag and operation RHS to register \;
        \For {each symbol $a \in \Sigma$} {
            $B = step \Xund on \Xund symbol (s, a)$ \;
            $C = epsilon \Xund closure (B)$ \;
            $O = transition \Xund regops (C, R, V)$ \;
            $P = precedence (C)$ \;
            $s' = add \Xund state (S, S_f, r_{\!f}, \varphi, C, P, O)$ \;
            $\delta(s, a) = (s', O)$ \;
        }
    }
    \BlankLine
    \Return TDFA $(\Sigma, T, S, S_f, s_0, R, r_{\!f}, \delta, \varphi)$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{add \Xund state \big( S, S_f, r_{\!f}, \varphi, C, P, O \big)} \smallskip$} {
    $X = \{ (q, r, l) \mid (q, r, \Xund, l) \in C \}$ \;
    $s = (X, P)$ \;
    \BlankLine
    \If {$s \in S$} {
        \Return $s$
    }
    \BlankLine
    \ElseIf {$\exists s' \in S$ such that $map(s, s', O)$} {
        \Return $s'$ \;
    }
    \BlankLine
    \Else {
        add $s$ to $S$ \;
        \If {$\exists (q, r, l) \in X$ such that $q = q_f$} {
            add $s$ to $S_f$ \;
            $\varphi(s) = final \Xund regops(r_{\!f}, r, l)$ \;
        }
        %\BlankLine
        \Return $s$ \;
    }
}
\vspace{2em}

\nonl\Fn {$\underline{map \big( (X, P), (X', P'), O \big)} \smallskip$} {
    \If {$X$ and $X'$ have different subsets of TNFA states \\
            \hphantom{\text{if }} or different lookahead tags for some TNFA state \\
            \hphantom{\text{if }} or precedence is different: $P \neq P'$ } {
        \Return $f\!alse$ \;
    }
    \BlankLine
    $M, M':$ empty maps from register to register \;
    %\BlankLine
    \For {each pair $(q, r, l) \in X$ and $(q, r', l) \in X'$} {
        \For {each $t \in T$} {
            \If {$history(l, t) = \epsilon$ or $t$ is a multi-tag} {
                $i = r[t], \; j = r'[t]$ \;
                %\BlankLine
                \If {both $M[i], M'[j]$ are undefined} {
                    $M[i] = j, \; M'[j] = i$ \;
                    %\BlankLine
                } \ElseIf {$M[i] \neq j$ or $M'[j] \neq i$} {
                    \Return $f\!alse$ \;
                }
            }
        }
    }
    \BlankLine
    \For {each operation $i \leftarrow \Xund$ in $O$} {
        replace register $i$ with $M[i]$ \;
        remove pair $(i, M[i])$ from $M$ \;
    }
    \BlankLine
    \For {each pair $(j, i) \in M$ where $j \neq i$} {
        prepend copy operation $i \leftarrow j$ to $O$ \;
    }
    \BlankLine
    \Return $topological \Xund sort(O)$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{precedence \big( C \big)} \smallskip$} {
    \Return vector $\{q \mid (q, \Xund, \Xund, \Xund)$ in $C \}$ \;
}

%\vfill\null
\columnbreak
\Indp
\SetNlSkip{-0.8em}

\nonl\Fn {$\underline{step \Xund on \Xund symbol \big( (X, P), a \big)} \smallskip$} {
    $B:$ empty sequence of configurations \;
    \BlankLine
    \For {$(q, r, l) \in X$ ordered by $q$ in the order of $P$} {
        \If {$\exists (q, a, p) \in \Delta \mid a \in \Sigma$} {
            append $(p, r, l, \epsilon)$ to $B$
        }
    }
    \BlankLine
    \Return $B$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{epsilon \Xund closure \big( B \big)} \smallskip$} {
    $C:$ empty sequence of configurations \;
    \BlankLine
    %push configurations in $B$ on stack in reverse order \;
    \For {$(q, r, h, \epsilon)$ in $B$ in reverse order} {
        push $(q, r, h, \epsilon)$ on stack \;
    }
    \BlankLine
    \While {stack is not empty} {
        pop $(q, r, h, l)$ from stack \;
        append $(q, r, h, l)$ to $C$ \;
        \For {each $(q, i, t, p) \in \Delta$ ordered by priority $i$} {
            \If {configuration with state $p$ is not in $C$} {
                push $(p, r, h, lt)$ on stack \;
            }
        }
    }
    \BlankLine
    \Return $\{ (q, r, h, l)$ in $C \mid q = q_f$ or \\
        \hphantom{\Return $\{ (q, r, h, l)$ in $C \mid$} $\exists (q, a, \Xund) \in \Delta$ where $a \in \Sigma \}$  \;
}
\vspace{2em}

\nonl\Fn {$\underline{transition \Xund regops \big( C, R, V \big)} \smallskip$} {
    $O:$ empty list of operations \;
    \BlankLine
    \For {each $(q, r, h, l) \in C$} {
        \For {each tag $t \in T$} {
            \If {$h_t = history(h, t) \neq \epsilon$} {
                $v = regop \Xund rhs(r, h_t, t)$ \;
                \BlankLine
                \If {$V[t][v]$ is undefined } {
                    $i = max \{R\} + 1$ \;
                    $R = R \cup \{i\}$ \;
                    $V[t][v] = i$ \;
                    append operation $i \leftarrow v$ to $O$ \;
                }
                \BlankLine
                $r[t] = V[t][v]$ \;
            }
        }
    }
    \BlankLine
    \Return $O$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{final \Xund regops \big( r_{\!f}, r, l \big)} \smallskip$} {
    $O:$ empty list of operations \;
    \BlankLine
    \For {each tag $t \in T$} {
        \If {$l_t = history(l, t) \neq \epsilon$} {
            append operation $r_{\!f}[t] \leftarrow regop \Xund rhs(r, l_t, t)$ to $O$ \;
        }
    }
    \BlankLine
    \Return $O$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{regop \Xund rhs \big( r, h_t, t \big)} \smallskip$} {
    \If {$t$ is a multi-valued tag} {
        \Return $r[t] \cdot h_t$
    } \Else {
        \Return the last element of $h_t$
    }
}
\vspace{2em}

\nonl\Fn {$\underline{history \big( h, t \big)} \smallskip$} {
    \Switch {$h$} {
        \lCase {$\epsilon$} {
            \Return $\epsilon$
        }
        \lCase {$\;\;\,t \cdot h'$} {
            \Return $\mathbf{p} \cdot history(h')$
        }
        \lCase {$-t \cdot h'$} {
            \Return $\mathbf{n} \cdot history(h')$
        }
        \lCase {$\;\,\,\Xund \cdot h'$} {
            \Return $history(h')$
        }
    }
}

\vfill\null
\end{multicols}
\vspace{1em}
\caption{Determinization of TNFA $(\Sigma, T, Q, q_0, q_f, \Delta)$.
}\label{alg_tdfa}
\end{algorithm}

\section{Optimizations}\label{section_optimizations}

In this section we describe optimizations and practical details that should be taken into account when implementing TDFA.
None of the optimizations is particularly complex or vital for TDFA operation,
but applied together and in the correct order they can make TDFA considerably faster and smaller.

\subsection{Fallback operations}

In practice it is often necessary to match the longest possible prefix of a string rather than the whole string.
This means that after matching a short prefix, TDFA may attempt to match a longer prefix;
and if that fails, it must fallback to the previous final state and restore the input position accordingly.
A final state is also a \emph{fallback state} if there are non-accepting paths out of it.
A path is non-accepting if does not go through another final state
(which may happen either because the input characters do not match, or due to a premature end of input).
%
For an ordinary DFA the only information that should be saved in a fallback state is the input position.
For TDFA it is also necessary to backup registers that may be clobbered on the non-accepting paths from the fallback state.
Backup operations should be added on transitions out of the fallback state,
and restore operations should be added on the fallback quasi-transition.
%
Algorithm \ref{alg_fallback} shows how to add such operations
(it assumes that fallback states and clobbered registers for each fallback state have already been identified).
Final registers are reused for backups, as by construction they are used only on the final quasi-transition.
\\

\begin{algorithm}[H] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\setstretch{0.9}
\small

\begin{multicols}{2}
\Indm

\nonl\Fn {$\underline{f\!allback \Xund regops \big( \big)} \smallskip$} {
    $\psi:$ undefined fallback function \;
    \BlankLine
    \For {each fallback state $s \in S$} {
        $O:$ empty list of register operations \;
        \BlankLine
        \For {each operation on quasi-transition $\varphi(s)$} {
            \If {append $i \leftarrow j \cdot h$ and $j$ is clobbered} {
                $backup \Xund regops(s, i, j)$ \;
                append operation $i \leftarrow i \cdot h$ to $O$
            }
            \ElseIf {copy $i \leftarrow j$ and $j$ is clobbered} {
                $backup \Xund regops(s, i, j)$
            }
            \Else {
                append a copy of this operation to $O$
            }
        }
        \BlankLine
        $\psi(s) = O$
    }
    \BlankLine
    \Return $\psi$ \;
}

\columnbreak

\nonl\Fn {$\underline{backup \Xund regops \big( s, i, j \big)} \smallskip$} {
    \For {each alphabet symbol $a \in \Sigma$} {
        $(s', O) = \delta(s, a)$ \;
        \If {exist non-accepting paths from $s'$} {
            append copy operation $i \leftarrow j$ to $O$
        }
    }
}

\vfill\null
\end{multicols}

\vspace{1em}
\caption{Adding fallback operations to TDFA
$(\Sigma, T, S, S_f, s_0, R, r_{\!f}, \delta, \varphi)$.
}\label{alg_fallback}
\end{algorithm}
\vspace{2em}

\subsection{Multi-valued tags as tries}

\subsection{Register optimizations}

TDFA induces a \emph{control flow graph} (CFG) with three kinds of nodes:
1) \emph{basic blocks} --- nodes that correspond to register operations on symbolic transitions,
2) \emph{final blocks} --- nodes that correspond to final register operations, and
3) \emph{fallback blocks} --- nodes that correspond to fallback register operations.
There is an arrow between two blocks in CFG if one is reachable from another in TDFA without passing through other register operations.
Additionally, fallback blocks have transitions to all blocks reachable by TDFA paths that may fall through to these blocks.
\\

Traditional compiler optimizations can be applied to CFG, resulting in significant reduction of registers and operations.
In the RE2C implementation we used the following optimization passes
(the number of repetitions $N=2$ was chosen impirically: subsequent passes didn't have any impact on the generated code for RE2C tests and benchmarks).

\begin{enumerate}[topsep = 0.5em]
    \item Compaction.
    \item Repeat $N$ times:
    \begin{enumerate}[label=\alph*.]
        \item Liveness analysis.
        \item Dead code elimination.
        \item Interference analysis.
        \item Register allocation with copy coalescing.
        \item Local operation reordering and deduplication.
    \end{enumerate}
\end{enumerate}

Pseudocode is shown on figures \ref{fig_opt1} and \ref{fig_opt2}.
Below we describe each pass.

\paragraph{Compaction.}
This pass is applied only once immediately after determinization.
It renames registers so that they occupy contiguous range of numbers with no ``holes''.
This is needed primarily to allow other optimization passes use registers as indices in data structures.
In our example registers $r_1$ to $r_5$ and $r_{16}$ to $r_{19}$ are removed by compaction,
reducing the total number of registers from 20 to 11 and the size of the liveness and interference matrices almost 2x and 4x respectively.
\\

\begin{algorithm}[H] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\begin{multicols}{2}
\setstretch{0.9}
\small

\Indm

\nonl\Fn {$\underline{optimizations \big( G \big)} \smallskip$} {
    $V = compaction(G)$ \;
    $G = renaming(G, V)$ \;
    \For {$i = \overline{1,2}$} {
        $L = liveness \Xund analysis(G)$ \;
        $dead \Xund code \Xund elimination(G, L)$ \;
        $I = inter\!f\!erence \Xund analysis(G, L)$ \;
        $V = register \Xund allocation (G, I)$ \;
        $renaming(G, V)$ \;
        $normalization(G)$ \;
    }
}
\vspace{2em}

\nonl\Fn {$\underline{renaming \big( G, V \big)} \smallskip$} {
    \For {each block $b$ in $G$} {
        \For {each operation in $b$} {
            \If {set operation $i \leftarrow v$} {
                rename $i$ to $V[i]$ \;
            }
            \If {copy or append operation $i \leftarrow j ...$} {
                rename $i$ to $V[i]$ and $j$ to $V[j]$ \;
            }
        }
    }
}
\vspace{2em}

\nonl\Fn {$\underline{liveness \Xund analysis \big( G \big)} \smallskip$} {
    $L:$ boolean matrix indexed by blocks and registers \;
    \BlankLine
    \For {each block $b$ in $G$} {
        \For {each register $i$ in $G$} {
            $L[b][i] = f\!alse$ \;
        }
    }
    \BlankLine
    \For {each final block $b$ in $G$} {
        \For {each final register $i$ in $G$} {
            $L[b][i] = true$ \;
        }
    }
    \BlankLine
    \While {$true$} {
        $fix = true$ \;
        \For {each basic block $b$ in $G$ in post-order} {
            $L_b = $ copy of row $L[b]$ \;
            \For {each successor $s$ of block $b$} {
                $L_s = $ copy of row $L[s]$ \;
                \For {each operation in $s$ in post-order} {
                    \If {set operation $i \leftarrow v$} {
                        $L_s[i] = f\!alse$ \;
                    }
                    \If {copy operation $i \leftarrow j$} {
                        \If {$L_s[i]$} {
                            $L_s[i] = f\!alse$ \;
                            $L_s[j] = true$ \;
                        }
                    }
                }
                \BlankLine
                \For {each register $i$ in $G$} {
                    $L_b[i] = L_b[i] \vee L_s[i]$
                }
            }
            \BlankLine
            \If {$L[b] \neq L_b$} {
                $L[b] = L_b$ \;
                $fix = f\!alse$ \;
            }
        }
        \lIf {$fix$} {$break$}
    }
    \BlankLine
    \For {each fallback block $b$ in $G$} {
        \For {each final register $i$ in $G$} {
            $L[b][i] = true$ \;
        }
        \BlankLine
        $L_b = $ copy of row $L[b]$ \;
        \For {each operation $i \leftarrow \Xund$ in $b$} {
            $L_b[i] = f\!alse$ \;
        }
        \For {each copy or append operation $\Xund \leftarrow j ...$ in $b$} {
            $L_b[j] = true$ \;
        }
        \BlankLine
        \For {each block $s$ in $G$ that may fall through to $b$} {
            \For {each register $i$ in $G$} {
                $L[s][i] = L[s][i]$ or $L_b[i]$ \;
            }
        }
    }
    \BlankLine
    \Return $L$ \;
}

\vfill\null

\columnbreak

\Indp
\SetNlSkip{-0.8em}

\nonl\Fn {$\underline{compaction \big( G \big)} \smallskip$} {
    $U:$ boolean vector indexed by registers \;
    $V:$ integer vector indexed by registers \;
    \BlankLine
    \For {each register $i$ in $G$} {
        $U[i] = f\!alse$ \;
    }
    \For {each block $b$ in $G$} {
        \For {each operation in $b$} {
            \If {set operation $i \leftarrow v$} {
                $U[i] = true$ \;
            }
            \If {copy or append operation $i \leftarrow j ...$} {
                $U[i] = U[j] = true$ \;
            }
        }
    }
    $n = 0$ \;
    \For {registers $i$ in $G$ such that $U[i]$} {
        $n = n + 1, \; V[i] = n$ \;
    }
    \BlankLine
    \Return $V$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{dead \Xund code \Xund elimination \big( G, L \big)} \smallskip$} {
    \For {each basic block $b$ in $G$} {
        $L_b = $ copy of row $L[b]$ \;
        \For {each operation $i \leftarrow \Xund$ in $b$ in post-order} {
            \If {$L_b[i]$} {
                \If {set operation $i \leftarrow v$} {
                    $L_b[i] = f\!alse$ \;
                }
                \If {copy operation $i \leftarrow j$} {
                    $L_b[i] = f\!alse$ \;
                    $L_b[j] = true$ \;
                }
            } \lElse {
                remove dead operation
            }
        }
    }
}
\vspace{2em}

\nonl\Fn {$\underline{inter\!f\!erence \Xund analysis \big( G, L \big)} \smallskip$} {
    $I:$ boolean matrix indexed by registers \;
    $V:$ vector of histories indexed by registers \;
    \BlankLine
    \For {each register $i$ in $G$} {
        \For {each register $j$ in $G$} {
            $I[i][j] = I[j][i] = f\!alse$ \;
        }
    }
    \BlankLine
    \For {each block $b$ in $G$} {
        \For {each copy or append operation $i \leftarrow j ...$ in $b$} {
            $V[j] = j$ \;
        }
        \BlankLine
        \For {each operation in $b$} {
            $I_b = $ copy of row $L[b]$ \;
            \BlankLine
            \If {set operation $i \leftarrow v$} {
                $V[i] = v$ \;
                $I_b[i] = f\!alse$ \;
            } \ElseIf {copy operation $i \leftarrow j$} {
                $V[i] = V[j]$ \;
                $I_b[i] = I_b[j] = f\!alse$ \;
            } \ElseIf {append operation $i \leftarrow j \cdot h$} {
                $V[i] = V[j] \cdot h$ \;
            }
            \BlankLine
            \For {operations $k \leftarrow \Xund$ in $b$ with $V[k] = V[i]$} {
                $I_b[k] = f\!alse$ \;
            }
            \BlankLine
            \For {registers $k$ in $G$ such that $I_b[k]$} {
                $I[i][k] = I[k][i] = true$ \;
            }
        }
    }
    \BlankLine
    \For {registers $i$ in $G$ not used in append operations} {
        \For {registers $j$ in $G$ used in append operations} {
            $I[i][j] = I[j][i] = true$ \;
        }
    }
    \BlankLine
    \Return $I$ \;
}

\vfill\null

\end{multicols}
\vspace{1em}
\caption{Register optimizations (part 1).}
\end{algorithm}

\begin{algorithm}[H] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\begin{multicols}{2}
\setstretch{0.9}
\small

\Indm

\nonl\Fn {$\underline{register \Xund allocation \big( G, I \big)} \smallskip$} {
    $V:$ vector of registers indexed by registers \;
    $B:$ vector of registers indexed by registers \;
    $S:$ vector of register sets indexed by registers \;
    \BlankLine
    \For {each register $i$ in $G$} {
        $B[i] = -1$ \;
        $S[i] = \emptyset$ \;
    }
    \BlankLine
    \For {each block $b$ in $G$} {
        \For {each operation in $b$} {
            \If {copy or append $i \leftarrow j ...$ and $i \neq j$} {
                $x = B[i], \; y = B[j]$ \;
                \If {$x = -1$ and $y = -1$} {
                    $B[i] = B[j] = i$ \;
                    $S[i] = \{i, j\}$ \;
                } \ElseIf {$x \neq -1$ and $y = -1$} {
                    \If {$\forall k \in S[x]: \neg I[k][j]$} {
                        $B[j] = x$ \;
                        $S[x] = S[x] \cup \{j\}$ \;
                    }
                } \ElseIf {$x = -1$ and $y \neq -1$} {
                    \If {$\forall k \in S[y]: \neg I[k][i]$} {
                        $B[i] = y$ \;
                        $S[y] = S[y] \cup \{i\}$ \;
                    }
                }
            }
        }
    }
    \BlankLine
    \For {registers $i$ in $G$ such that $B[i] = i$} {
        \For {registers $j$ in $G$ such that $B[j] = j$ and $j > i$} {
            \If {$\forall i \in S[x], j \in S[y]: \neg I[i][j]$} {
                $B[y] = x$ \;
                $S[x] = S[x] \cup S[y]$ \;
                $S[y] = \emptyset$ \;
            }
        }
    }
    \BlankLine
    \For {registers $i$ in $G$ such that $B[i] = -1$} {
        \If {$\exists j$ in $G: B[j] = j$ and $\forall k \in S[j]: \neg I[i][k]$} {
            $B[i] = j$ \;
            $S[j] = S[j] \cup \{i\}$ \;
        }
    }
    \BlankLine
    $n = 0$ \;
    \For {registers $i$ in $G$ such that $B[i] = i$} {
        $n = n + 1$ \;
        \For {registers $j \in S[i]$} {
            $V[j] = n$ \;
        }
    }
    \BlankLine
    \Return $V$ \;
}

\vfill\null

\columnbreak

\nonl\Fn {$\underline{normalization \big( G \big)} \smallskip$} {
    \For {each block $b$ in $G$} {
        \For {each contiguous range $O$ of set operations} {
            $remove \Xund duplicates(O)$ \;
            $sort(O)$ \;
        }
        \For {each contiguous range $O$ of copy operations} {
            $remove \Xund duplicates(O)$ \;
            $topological \Xund sort(O)$ \;
        }
        \For {each contiguous range $O$ of append operations} {
            $remove \Xund duplicates(O)$ \;
        }
    }
}
\vspace{2em}

\nonl\Fn {$\underline{topological \Xund sort \big( O \big)} \smallskip$} {
    $I:$ vector of in-degree indexed by registers \;
    \For {each copy or append operation $i \leftarrow j ...$ in $O$} {
        $I[i] = I[j] = 0$ \;
    }
    \For {each copy or append operation $\Xund \leftarrow j ...$ in $O$} {
        $I[j] = I[j] + 1$ \;
    }
    \BlankLine
    $O':$ empty list of operations \;
    $nontrivial \Xund cycle = f\!alse$ \;
    \BlankLine
    \While {$O$ is not empty} {
        \For {each operation $i \leftarrow \Xund$ in $O$} {
            \If {$I[i] = 0$} {
                remove operation from $O$ and append to $O'$ \;
                \If {this is a copy/append operation $i \leftarrow j ...$} {
                    $I[j] = I[j] - 1$ \;
                }
            }
        }
        \If {nothing added to $O'$ but $O$ is not empty} {
            \If {$\exists$ operation $i \leftarrow j ...$ in $O$ such that $i \neq j$} {
                $nontrivial \Xund cycle = true$
            }
            append $O$ to $O'$ \;
            $break$ (only cycles left)
        }
    }
    \BlankLine
    $O = O'$ \;
    \Return $\not nontrivial \Xund cycle$ \;
}
\vspace{2em}

\nonl\Fn {$\underline{remove \Xund duplicates \big( O \big)} \smallskip$} {
    \For {each operation $o$ in $O$} {
        \For {each subsequent operation $o' = o$ in $O$} {
            remove duplicate operation $o'$
        }
    }
}

\vfill\null

\end{multicols}
\vspace{0.5em}
\caption{Register optimizations (part 2).}\label{alg_opt2}
\end{algorithm}

\subsection{Minimization}

Minimization can be

\FloatBarrier

\subsection{Fixed tags}

\newcommand\nan{N\!an}
\newcommand\nobasetag{-1}%{N\!otag}

If a pair of tags is within fixed distance from each other, there is no need to track both of them:
the value of one tag can be computed from the value of the other tag one by adding a fixed offset.
For example, in RE in $(1a2)^*3(a|4b)5b^*$ tags $t_1$ and $t_2$ are within one symbol from each other,
so the value of $t_1$ can be computed as nil if $t_2$ is nil, or $t_2 - 1$ otherwise.
Likewise $t_3$ can be computed as $t_5 - 1$ (although there are multiple different paths through $(a|4b)$, they all have the same length).
Registers and register operations associated with fixed tags $t_1$ and $t_3$ are not needed, resulting in TDFA on figure \ref{fig:tdfa_fixopt}.
\\

\begin{algorithm}[H] \DontPrintSemicolon \SetKwProg{Fn}{}{}{} \SetAlgoInsideSkip{medskip}
\setstretch{0.9}
\small

\Indm

\nonl\Fn {$\underline{fixed \Xund tags \big( e, t, d, k \big)} \smallskip$} {
    \If {$e = \epsilon$} {
        \Return $t, d, k$
    }
    \BlankLine
    \ElseIf {$e = a \in \Sigma$} {
        \Return $t, d + 1, k + 1$
    }
    \BlankLine
    \ElseIf {$e = e_1 | e_2$} {
        $\Xund, \Xund, k_1 = fixed \Xund tags(e_1, \nobasetag, \nan, 0)$ \;
        $\Xund, \Xund, k_2 = fixed \Xund tags(e_2, \nobasetag, \nan, 0)$ \;
        \If {$k_1 = k_2$} {
            \Return $t, d + k_1, k + k_1$
        }
        \Return $t, \nan, \nan$
    }
    \BlankLine
    \ElseIf {$e = e_1 e_2$} {
        $t_2, d_2, k_2 = fixed \Xund tags(e_2, t, d, k)$ \;
        $t_1, d_1, k_1 = fixed \Xund tags(e_1, t_2, d_2, k_2)$ \;
        \Return $t_1, d_1, k_1$
    }
    \BlankLine
    \ElseIf {$e = e_1^{n, m}$} {
        $\Xund, \Xund, k_1 = fixed \Xund tags(e_1, \nobasetag, \nan, 0)$ \;
        \If {$n = m$} {
            \Return $t, d + n * k_1, k + n * k_1$
        }
        \Return $t, \nan, \nan$
    }
    \BlankLine
    \ElseIf {$e = t_1 \in T$} {
        \If {$t \neq \nobasetag$ and $d \neq \nan$} {
            mark $t_1$ as fixed on $t$ with distance $d$ \;
            \Return $t, d, k$
        }
        \Return $t_1, 0, k$
    }
}

\vspace{1em}
\caption{Fixed tags optimization.}\label{alg_fixed_tags}
\end{algorithm}
\vspace{2em}

Algorithm \ref{alg_fixed_tags} finds fixed tags by performing top-down structural recursion on RE.
It has four parameters: $e$ is the current sub-RE, $t$ is the current base tag,
$d$ is the distance to base tag, and $k$ is the distance to the start of the current level.
Levels are parts of RE where any two points either both match or both do not match.
A level increases on recursive descent into alternative or repetition subexpressions, but not concatenation.
Tags on different levels should not be fixed on each other, even if they are within fixed distance on any path that goes through both of them,
because there are paths that go through only one tag (so the other one is nil).
Tag value $-1$ denotes the absence of base tag: when descending to the next level initially there is no base tag, and the first tag on the current level becomes the base.
One exception is the top level, where the initial base tag should be a special value denoting the rightmost position (which is always known at the end of match).
The algorithm recursively returns the new base tag, the updated distance to base tag, and the updated level distance.
Special distance value $\nan$ (not-a-number) is understood to be a fix-point in arithmetic expressions: any expression involving $\nan$ amounts to $\nan$.


\section{Example}\label{section_example}

\begin{figure}%[t!]
\includegraphics[width=\linewidth]{img/tdfa_construction.pdf}
\vspace{0.1em}
\caption{
Example for RE $(1a2)^*3(a|4b)5b^*$: TNFA, simulation on string $aab$, determinization, TDFA.
%TDFA construction for RE $(1a2)^*3(a|4b)5b^*$\\
%(top: TNFA, middle: determinization process, bottom: the resulting TDFA with final registers $r_6$ to $r_{10}$).
}\label{fig:tdfa_construction}
\end{figure}

\FloatBarrier

\begin{figure}[h!]
\includegraphics[width=\linewidth]{img/tdfa_optimized.pdf}
\vspace{0.5em}
\caption{
Register optimizations for TDFA on figure \ref{fig:tdfa_construction}. \\
Top to bottom: initial CFG,
CFG after compaction with per-block liveness information and interference table,\\
CFG on the second round of optimizations,
optimized TDFA with final registers $r_1$ to $r_5$.
}\label{fig:tdfa_regopt}
\end{figure}

\FloatBarrier

\includegraphics[width=\linewidth]{img/tdfa_opttagfix.pdf}
\vspace{0.5em}
\captionof{figure}{
TDFA with fixed tags optimization. \\
Fixed tags are $t_1 = (\mathbf{n} \text{ if } t_2 = \mathbf{n} \text{ else } t_{2} - 1)$ and $t_3 = t_{4} - 1$. \\
Tags $t_2$, $t_4$, $t_5$ correspond to final registers $r_1$, $r_2$, $r_3$.
}\label{fig:tdfa_fixopt}

\FloatBarrier

\section{Registerless TDFA}\label{subsection_regless}

\section{Evaluation}\label{section_evaluation}

\section{Conclusions}\label{section_conclusions}

\begin{thebibliography}{99}

\bibitem{Lau00}
    Ville Laurikari,
    \textit{NFAs with tagged transitions, their conversion to deterministic automata and application to regular expressions},
    Proceedings Seventh International Symposium on String Processing and Information Retrieval, 2000. SPIRE 2000,
    pp. 181-187,
    URL: \url{http://laurikari.net/ville/spire2000-tnfa.pdf}.

\bibitem{Kuk07}
    Chris Kuklewicz,
    \textit{Regular expressions/bounded space proposal},
    \url{http://wiki.haskell.org/index.php?title=Regular_expressions/Bounded_space_proposal&oldid=11475},
    2007.

\bibitem{Tro17}
    Ulya Trofimovich,
    \textit{Tagged Deterministic Finite Automata with Lookahead},
    arXiv:1907.08837 [cs.FL],
    2017.

\bibitem{RE2C}
    RE2C, a lexer generator for C, C++, Go and Rust.
    Source code: \url{https://github.com/skvadrik/re2c}.
    Official website: \url{https://re2c.org},

\bibitem{BorTro19}
    Angelo Borsotti, Ulya Trofimovich,
    \textit{Efficient POSIX Submatch Extraction on NFA},
    preprint, 2019,
    URL: \url{https://re2c.org/2019_borsotti_trofimovich_efficient_posix_submatch_extraction_on_nfa.pdf}.

\bibitem{Gra15}
    Niels Bj{\o}rn Bugge Grathwohl,
    \textit{Parsing with Regular Expressions \& Extensions to Kleene Algebra},
    DIKU, University of Copenhagen,
    2015.

\bibitem{Kle51}
    Stephen Cole Kleene,
    \textit{Representation of events in nerve nets and finite automata},
    RAND Project US Air Force,
    1951.

\bibitem{Koz94}
    Dexter Kozen,
    \textit{A completeness theorem for {Kleene} algebras and the algebra of regular events},
    Elsevier,
    Information and computation,
    vol. 110 (2)
    pp. 366-390,
    1994.

\bibitem{Cho18}
    Mohammad Imran Chowdhury,
    \textit{staDFA: An Efficient Subexpression Matching Method},
    Master thesis,
    Florida State University,
    2018.

\bibitem{Ragel}
    Ragel State Machine Compiler.
    Official website: \url{https://www.colm.net/open-source/ragel},

\end{thebibliography}

\end{document}

